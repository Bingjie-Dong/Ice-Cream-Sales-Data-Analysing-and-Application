---
title: "STAT0006 ICA 3"
author: 'Student numbers: 21000790, 20058715, 20017151, 20181056'
subtitle: Group 86
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

### Introduction to the data

``` {r, echo=FALSE, include = FALSE}
#setwd("~/Downloads/Everything you need for ICA 3-20221231")
# importing data set
ic <- read.csv('ice_creams.csv')

# identifying and removing NA values
which(is.na(ic), arr.ind = TRUE)
ic <- ic[-c(271,249,177),]

# summary of ic
summary(ic)

```
```{r load-packages, include=FALSE}
library(scales)
```

**Original Dataset and Adjustment**

The given dataset `icecream.csv` includes data on 314 weekly sales of various ice cream brands in a supermarket chain over the past five years, each linked with data on 10 corresponding variables. It contained 3 sales records with missing values, which were removed, resulting in a modified dataset with 311 records. The number of ice creams sold per week ranges from 0 to 2444, with a mean of 530.4 ice creams.

**Variables Interpretation**

The variables `brand`, `brand_competitors`, `distance`, `holiday`, `milk`, `promotion`, `store_type`, `temperature`, `wind`, and `year` represent, respectively, the brand of the ice cream being sold; the number of other ice cream brands available in the store during that week; the distance (in miles) to the nearest another supermarket; whether there was a national bank holiday during the week; the national average wholesale price of milk during the week; whether there was a promotion campaign for this brand of ice cream during that week; the size of the store (Small, Medium, or Large); the average weekly store temperature (in °C); the average weekly wind speed at the store (in knots); and the year in which the sales were recorded. 

**Approach**

The aim of this analysis is to determine the extent to which the 10 factors influence the sales of a particular brand of ice cream. 

Figure 1.1 illustrates the relationship between the number of ice creams sold and the variables `brand` and `store_type`. The first plot indicates that ice cream from Brand A appears to be more popular than the other brands, while Brand B has moderate popularity, and Brand C seems to have the lowest popularity. The second plot shows that as the size of store decreases, the number of ice creams sold also decreases.

```{r Fig1.1, echo=FALSE, warning=FALSE, fig.height=4, fig.width=10,  fig.align='center', fig.cap="Figure 1.1: The boxplots show the number of ice cream sold each week in a store plotted against the categorical variables brand and storesize, with orange dots representing the mean number of sales in each category."}

par(mfrow=c(1,2))

boxplot(ic$sales~ic$brand, ylab = "Sales (No. of Ice Creams)", xlab = "Brand" , col = "grey", main = "Sales & Type of Brand",cex.axis = 0.8)
points(c(mean(ic$sales[ic$brand=="BrandA"]), 
         mean(ic$sales[ic$brand=="BrandB"]),
         mean(ic$sales[ic$brand=="BrandC"])),
       pch=16, cex=1.5, col="orange")

boxplot(ic$sales~ic$store_type, ylab = "Sales (No. of Ice Creams)", xlab = "Size of Store", col = "grey", main = "Sales & Size of store", cex.axis = 0.8)
points(c(mean(ic$sales[ic$store_type=="Large"]),
         mean(ic$sales[ic$store_type=="Medium"]),
         mean(ic$sales[ic$store_type=="Small"])),
       pch=16, cex=1.5, col="orange")

```

Figure 1.2 illustrates the relationship between the number of ice creams sold and the variables `promotion` and `holiday`. The first plot suggests that the weekly sales of ice cream tend to be higher when there is a promotion campaign for the particular brand, compared to weeks without such campaigns. Additionally, the second plot shows that there is a slight increase in the number of ice creams sold during weeks with national bank holidays, compared to weeks without such holidays.

```{r Fig1.2, echo=FALSE, warning=FALSE, fig.width=10, fig.align='center', fig.cap="Figure 1.2: Boxplots of the weekly sales amount against the binary variables promotionand holiday."}
par(mfrow=c(1,2))

boxplot(ic$sales~ic$promotion, ylab = "Sales (No. of Ice Creams)", xlab = "Y / N Promotion", col = "grey", main = "Sales & Whether there is Promotion", names = c("Without Promotion", "With Promotion"), cex.axis = 0.8)
points(c(mean(ic$sales[ic$promotion=="N"]),
         mean(ic$sales[ic$promotion=="Y"])),
         pch=16, cex=1.5, col = "orange")

boxplot(ic$sales~ic$holiday, ylab = "Sales (No. of Ice Creams)", xlab = "Y / N Bank Holiday" ,col = "grey", main = "Sales & Whether there is Bank Holiday", names = c(" ", " "), cex.axis = 0.8)
points(c(mean(ic$sales[ic$holiday=="N"]), 
         mean(ic$sales[ic$holiday=="Y"])),
         pch=16, cex=1.5, col = "orange")
axis(1,at = c(1,2), labels = c("Without Bank Holiday", "With Bank Holiday"),cex.axis = 0.8)
```

Figures 1.3 and 1.4 do not appear to demonstrate a clear linear relationship between the variables `sales` and `year`, or `sales` and `brand_competitors`, respectively. Therefore, it may be necessary to exclude the variables `year` and `brand_competitors` when constructing a normal linear regression model for ice cream sales.

```{r Fig1.3, echo=FALSE, warning=FALSE,fig.width=10, fig.align='center', fig.cap="Figure 1.3: Boxplots of the weekly ice cream sales amount with respect to the year the sales were recorded."}
boxplot(ic$sales~ic$year, ylab = "Sales (No. of Ice Creams)", xlab = "Year", col = "grey", main = "Sales between 2018 and 2022", cex.axis = 0.8)
points(c(mean(ic$sales[ic$year== 2018]),
        mean(ic$sales[ic$year== 2019]),
        mean(ic$sales[ic$year== 2020]),
        mean(ic$sales[ic$year== 2021]),
        mean(ic$sales[ic$year== 2022])),
       pch=16, cex=1.5, col="orange")

```

```{r Fig1.4, echo=FALSE, fig.width=10, fig.align='center', fig.cap="Figure 1.4: Boxplots of the number of ice cream sold with respect to number of ice cream brand competitors."}
boxplot(ic$sales~ic$brand_competitors, ylab = "Sales (No. of Ice Creams)", xlab = "No. of Competitors", col = "grey", main = "Sales & No of Competitors", cex.axis = 0.8)
points(c(mean(ic$sales[ic$brand_competitors== 3]),
        mean(ic$sales[ic$brand_competitors== 4]),
        mean(ic$sales[ic$brand_competitors== 5]),
        mean(ic$sales[ic$brand_competitors== 6]),
        mean(ic$sales[ic$brand_competitors== 7]),
        mean(ic$sales[ic$brand_competitors== 8]),
        mean(ic$sales[ic$brand_competitors== 9])),
       pch=16, cex=1.5, col="orange")
```

The top-left plot in Figure 1.5 reveals a weak, yet positive, linear relationship between `sales` and `distance`, while the top-right plot illustrates a clear positive linear relationship between `sales` and `temperature`. The relationships between `sales` and the variables `milk` and `wind` do not exhibit linearity, so it may be advisable to exclude both variables when constructing a linear model.

```{r Fig1.5, echo=FALSE, warning=FALSE, fig.width=10, fig.height=10, fig.align='center', fig.cap="Figure 1.5: Scatterplots that visualize the relationship between the ice cream sales and each of the variables distance (top-left), temperature (top-right), milk (bottom-left) and wind (bottom-right)."}
par(mfrow=c(2,2))

plot(ic$sales~ic$distance, ylab = "Sales (No. of Ice Creams)", xlab = "Distance to the nearest other supermarket (miles)", main = "Sales & Distance", pch = 20, col=alpha("darkblue",0.4))

plot(ic$sales~ic$temperature, ylab = "Sales (No. of Ice Creams)", xlab = "Average Weekly Temperature (°C) ", main = "Sales & Temperature", pch = 20, col=alpha("darkblue",0.4))

plot(ic$sales~ic$milk, ylab = "Sales (No. of Ice Creams)", xlab = "Average Weekly Price of Milk (pence/litre)", main = "Sales & Milk", pch = 20, col=alpha("darkblue",0.4))

plot(ic$sales~ic$wind, ylab = "Sales (No. of Ice Creams)", xlab = "Average Weekly Speed of Wind (knots)", main = "Sales & Wind", pch = 20, col=alpha("darkblue",0.4))
#m<-lm(sqrt(ic$distance)~ic$sales)
#print(summary(m))
```

### Model Building and Checking

**Step 1: Select important covariates for normal linear model**

Due to unclear linear relationship with `sales`, we decided to omit the covariates `year`, `wind`, `milk` and `brand_competitors` in Model 1. Model 1 is as below:

```{r Model 1, echo=FALSE}
#MODEL 2 REMOVED MILK AND WIND AND YEAR, ADDED INTERACTION

model1 <- lm(sales ~ 
               brand
              +holiday
              +promotion
              +store_type
              +temperature
              +distance
              , data = ic)
summary(model1)

```

The p-values of `store_typeMedium` and `store_typeSmall` in Model 1 are large, thus we might want to remove `store_type` from our model, keeping all the other covariates. However, Figure 1.1 suggests that `store_type` might influence `sales`, since ice cream sales increase as store size increases. Therefore, we kept `store_type` in the model and looked for further interactions between `store_type` and other covariates.

**Step 2: Add suitable interaction terms**

The relationship between `store_type` and `sales` might be more complex and might be dependent on other factors, such as the distance to the nearest store, the brand of ice cream being sold or whether there was a holiday or not. We therefore consider including interaction terms for `store_type*distance`, `store_type*brand` and `store_type*holiday`. The small p-value for `store_typeMedium` and `store_typeSmall` in the resulting Model 2 is a sufficient evidence for keeping `store_type` in the model. Model 2 is as below:

```{r Model 2, echo=FALSE}
model2 <- lm(sales~ 
              +holiday
              +temperature
              +promotion
              +distance*store_type
              +store_type*brand
              +store_type*holiday
             , data = ic)
summary(model2)
```

+ Holiday and Store Type

In Figure 2.1, there is an unclear distinction among the different store types when considering the effects of `holiday` on `sales`. Therefore, there is no strong evidence to keep `store_type * holiday` in the model.

```{r Fig2.1 Holiday*Type, echo=FALSE, fig.cap='Figure 2.1: Sales against Yes(Y) or No(N) Holiday among store types', fig.align='center'}
library(ggplot2)
df<-data.frame(x=ic$holiday,y=ic$sales, color = factor(ic$store_type))
ggplot(df,aes(x=x,y=y,color=color))+geom_point()+geom_jitter()+labs(x = "Y/N Bank Holiday", y = "Sales (No. of ice cream)")
```

+ Distance and Store Type

Based on the plot below, the data points dependent on each store type seem to behave in a different slope when plotting `distance` against `sales`. For this reason, there is evidence supporting the claim that the effect of `distance` on `sales` depends on `store_type`. 

```{r Fig2.2 Distance*Type, echo=FALSE, include=TRUE, fig.cap='Figure 2.2: Sales against distance, among store types', fig.align='center'}
plot(ic$sales~ic$distance, ylab = "Sales (No. of ice creams)", xlab = "Distance to the nearest other supermarket (miles)", pch = 20, col = factor(ic$store_type))
legend("topleft",legend=unique(factor(ic$store_type)),col = unique(factor(ic$store_type)), pch=20)
```

+ Brand and Store Type

From the figure below, the data points dependent on `Brand` appear to be randomly distributed in medium and small store categories. However, a clear ordering in sales appears in the large store category (Brand A > Brand B > Brand C), which is an evidence for retaining `brand*store_type` in the model. Therefore, only `store_type * distance` and `store_type * brand` are added as interactions.


```{r Fig 2.2.2 Brand&Type, echo=FALSE, include=TRUE, fig.cap='Figure 2.2: Sales against store type among brands', fig.align='center'}
library(ggplot2)
df<-data.frame(x=ic$store_type,y=ic$sales, color = factor(ic$brand))
ggplot(df,aes(x=x,y=y,color=color))+geom_point()+geom_jitter()+labs(x = "Sizes of store", y = "Ice cream Sales") + theme(plot.title = element_text(hjust = 0.5))

```


**Step 3: Finalise interaction term selection**

Interaction term `store_type * brand` is removed. The resulting Model 3 is below:


```{r Model 3, echo=FALSE}

model3 <- lm(sales~ 
              temperature
              +promotion
              +holiday
              +distance * store_type
              +store_type*brand, data = ic)
summary(model3)

```

*Assumption Evaluation*

It is crucial to verify the assumptions of an ordinary least squares model using diagnostic plots to ensure its validity. These assumptions include: the normality, the constant variance and independence of the error terms. It is also advisable to examine multicollinearity afterwards.

- Normality: Regarding the assumption of normality, the QQ-plot in Figure 3.2 indicates that the error term is approximately normally distributed, although the tails are slightly heavier than expected under the normality assumption. Consequently, there are no major normality concerns.

```{r Model 3 Normality Fig3.2, echo=FALSE, fig.cap='Figure 3.2 ', fig.align='center'}
#normality of error the term
model3_stdres<-rstandard(model3)
qqnorm (model3_stdres, main="", ylab = "Standardised Residuals", xlab = "Quantiles of N(0,1)", pch = 20, col=alpha("darkblue",0.4))
qqline (model3_stdres)
```

- *Homoscedasticity and Independence:* 
These assumptions are not violated if there is no systematic pattern in Standardised Residuals-Fitted Values plot. The left side of Figure 3.3 reveals a linear pattern in a range of fitted values, particularly where the fitted values of `sales` are negative. Transformations can be applied to check if homoscedasticity can be resolved.


```{r Model 3 Homoscedasticity Fig 3.3, echo=FALSE, fig.cap='Figure 3.3', fig.align='center'}
#homoscedasticity of the error term
model3_fitted<-fitted(model3)
plot(model3_fitted, model3_stdres, xlab="Fitted values", ylab="Standardised Residuals", pch = 20, col=alpha("darkblue",0.4))
abline(a=0, b=0)

```

+ *Multicollinearity:*
Multicollinearity can be assessed by evaluating the variance inflation factors (VIFs) for Model 3. As all VIF < 5, indicating that multicollinearity is not a concern. This is further supported by the fact that the estimated coefficients are relatively stable, meaning that they do not change much when the model is retrained.

+ *Potentially omitted important covariates:*
No systematic relationship is observed in the standardised residual values against the omitted `wind` and `milk` (Figure 3.4), suggesting there is no need to include these variables in the model.


```{r Fit M3, echo=FALSE, include=TRUE, fig.width=10, fig.height=4, fig.cap='Figure 3.4: Model 3 - Investigation of Left Out Variables' ,fig.align='center', fig.align='center'}

par(mfrow=c(1,2))

plot(ic$milk, model3_stdres, xlab="Average Weekly Price of Milk (pence/litres)", ylab="Standardised residuals",main="Model 3 Standardised Residuals & Milk",pch = 20, col=alpha("darkblue",0.4))
abline(a=0, b=0, col = 'red')

plot(ic$wind, model3_stdres, xlab="Average Weekly Speed of Wind (knots)", ylab="Standardised residuals",main="Model 3 Standardised Residuals & Wind",pch = 20, col=alpha("darkblue",0.4))
abline(a=0, b=0, col = 'red')

```


In summary, Model 3 exhibits a strong fit for the observed data, but some predicted values are negative, and there is a linear pattern in the plot of standardised residuals against fitted values, suggesting potential concerns about homoscedasticity and independence.

**Step 4: Transformation of Response Variable**

To address the homoscedasticity issue in Model 3, we applied Box-Cox transformation to `sales` to the variance. The optimal transformation function is determined by identifying the value of lambda that maximises the log-likelihood, as depicted in Figure 3.5 below. 

```{r BOX COX Fig 2.3,echo=FALSE, fig.height=6, fig.width=6, fig.cap='Figure 3.5: Box-Cox Transformation after Model 3',fig.align='center'}
library(MASS)
boxcox(I(sales+0.5) ~ 
               brand
              +brand_competitors
              +distance
              +holiday
              +store_type
              +temperature
              +distance * store_type
              +store_type*brand, data = ic)
```

The optimal value of lambda is approximately 0.5, indicating that the suitable function for transforming `sales` is the square root function. Note that Box-Cox transformation only works on strictly positive values, yet there are several zero sales observations in the dataset. Adding a constant can make them positive, without changing the original data distribution. Model 4 is as below:

```{r M4, echo=FALSE, include= TRUE}
model4 <- lm(sqrt(sales)~ 
              +temperature
              +promotion
              +holiday
              +distance * store_type
              +store_type*brand, data = ic)
summary(model4)

```

+ *Assumptions Evaluation*

Model 4 exhibits normality to a satisfactory extent, despite the fatter tails compared to Model 3. The transformation of `sales` with square root function did ensure the positive range of sales values. However, the linear pattern was not removed in Figure 3.6, indicating that the homoscedasticity issue in Model 3 was not fully resolved. 


```{r M4 Normality Fig3.5, echo=FALSE, fig.width=10, fig.height=4, include=TRUE,fig.cap='Figure 3.6: Assumption Diagnosis for Model 4', fig.align='center'}

par(mfrow=c(1,2))

#normality of error the term
model4_stdres<-rstandard(model4)
qqnorm (model4_stdres, main="", ylab = "Standardised Residuals", xlab = "Quantiles of N(0,1)",col=alpha("darkblue",0.4), pch = 20)
qqline (model4_stdres, col = 'red')

#homoscedasticity of the error term
model4_fitted<-fitted(model4)
plot(model4_fitted, model4_stdres, xlab="Fitted values", ylab="Standardised residuals",col=alpha("darkblue",0.4), pch = 20)
abline(a=0, b=0, col = 'red')

```

## Comparing fit of all models

Model 3's performance appears to be satisfactory when compared to its observed ice cream sales data plot to all other models, depicted in Figure 3.7. 

```{r Model3 Fit, echo=FALSE, include=TRUE,fig.width=14, fig.height=10, echo = FALSE, fig.cap='Figure 3.7: Model Fit Diagnosis' ,fig.align='center'}

par(mfrow=c(2,2))

predicted1 <- fitted(model1)
plot(ic$sales~predicted1, xlab = "Predicted sales", ylab = "Observed sales", xlim = c(0,2500), main = "Model 1", pch=16, asp = 0.6, col=alpha("darkblue",0.4))
abline(a=0, b=1, col = "red")
grid(nx = NULL, ny = NULL,
     lty = 2,      # Grid line type
     col = "gray", # Grid line color
     lwd = 1)      # Grid line width

predicted2 <- fitted(model2)
plot(ic$sales~predicted2, xlab = "Predicted sales", ylab = "Observed sales", xlim = c(0,2500), main = "Model 2", pch=16, asp = 0.6, col=alpha("darkblue",0.4))
abline(a=0, b=1, col = "red")
grid(nx = NULL, ny = NULL,
     lty = 2,      # Grid line type
     col = "gray", # Grid line color
     lwd = 1)      # Grid line width

predicted3 <- fitted(model3)
plot(ic$sales~predicted3, xlab = "Predicted sales", ylab = "Observed sales", xlim = c(0,2500), main = "Model 3", pch=16, asp = 0.6, col=alpha("darkblue",0.4))
abline(a=0, b=1, col = "red")
grid(nx = NULL, ny = NULL,
     lty = 2,      # Grid line type
     col = "gray", # Grid line color
     lwd = 1)      # Grid line width

predicted4 <- fitted(model4)
plot(ic$sales~I(predicted4^2), xlab = "Predicted sales", ylab = "Observed sales", xlim = c(0,2500), main = "Model 4", pch=16, asp = 0.6, col=alpha("darkblue",0.4))
abline(a=0, b=1, col = "red")
grid(nx = NULL, ny = NULL,
     lty = 2,      # Grid line type
     col = "gray", # Grid line color
     lwd = 1)      # Grid line width

```

The fit was highly improved in Model 2 from Model 1, supported by the increase in the R-Squared value (0.6088 < 0.8939). When comparing Model 2 to Model 3, Model 3 has fewer overvalued points in the [500,1000] range and appears to fit the lower sales points more strongly.

Evaluating the fit of Model 4 raises concerns in higher sales values, when the fit starts to form a curved relationship, although its R-Squared value improved from Model 3 (0.8718 > 0.8263). Thus, Model 3 remains the most suitable predictions for the Ice Cream Sales dataset.

### Conclusion

Model 3 suggests the expected ice cream sale is around 800 units for the reference group. The chosen reference group in our final model (Model 3) is `BrandA`, `N` for no promotion, and the store_type `Large`.

`Promotion`: Running a promotion can have a positive effect on weekly sales, which is expected to be 210.7-unit higher than when there is no promotion, holding remaining covariates constant.

`Temperature`: People buy ice cream when the weather is hotter, which is at 44 units increase of weekly sales per 1°C increase, holding  all other covariates constant.
 
`Distance` * `store_type`: Looking at the coefficients of `distance:store_typeMedium` and `distance:store_typeSmall`, the final model suggests that when `distance` increases by one mile, the increase in ice cream sales in small stores > medium stores > large stores. 

Figure 4.1 shows that small and medium stores tend to have very low sales when the distance to the nearest stores is small. This could be explained by reasons such as people would want to go to larger stores at such a distance so that they can shop for more other goods/ have more ice cream options. When this distance increases, people might adhere to the current store for convenience. 

Also, the increase in ice cream sales is more sticky in large stores because shoppers have more options within that shop. They will be less willing to travel to nearby stores, thus ice cream sales in large stores are less influenced by the distance to the nearest store.

```{r Fig4.1 Dis*Type, fig.cap='Figure 4.1: Sales against distance among store types', echo=FALSE, include=TRUE, fig.align='center'}
plot(ic$sales~ic$distance, ylab = "Sales (no of ice creams)", xlab = "Distance to the nearest other supermarket (miles)", pch = 20, col = factor(ic$store_type))
legend("topleft",legend=unique(factor(ic$store_type)),col = unique(factor(ic$store_type)), pch=20)
abline(model3$coef[1], model3$coef[5], lwd=3, lty=2, col="black")
abline((model3$coef[1]+model3$coef[6]), (model3$coef[5]+model3$coef[10]), lwd=3, lty=2, col="red")
abline((model3$coef[1]+model3$coef[7]), (model3$coef[5]+model3$coef[11]), lwd=3, lty=2, col="green")

```

`store_type*brand`: Figure 2.2 showed consumers seem to care less about the brand of ice cream when they are shopping in small stores, the points don’t display a clear separation. However, in larger stores, there is a clearer distinction between the sales of ice cream: Brand A > Brand B > Brand C. This could be explained by shopping tastes or availability problems in smaller stores.

### Discussion of limitations 

**Data**

Even though the dataset already included the most important factors, some important variables might have been left out. Some possibilities can be the population density in the store area, marketing spending or demographic factors.

**Model**

From the bottom-left plot in Figure 3.7, most of the negative predicted values are aligned with the observed sales values at 0. From Figure 3.3, these values also form the linear pattern, causing concerns about the homoscedasticity and independence assumptions. 

We investigated the observations in the dataset where sales are zero, which seem reasonable: mostly among small and medium stores, the nearest stores are within 1-mile, there is no promotion etc. There appears to be no systematic problems with these data, we call these the ‘empty season’, where sales happen to be zero.

We also investigated the extreme points in the dataset. The model also does not fit as strongly with higher values of sales, particularly the three extreme values at  ‘peak seasons’ where sales are over 2000. These values have high values of sales, and after investigation, we realised that they seem to be valid because they are observed in large stores, and during holiday seasons. Although they might pull the fitted hyperplane towards the higher values, these values only make up less than 1% of the observations, they are therefore not a major concern.


**Total word count:** 1989
